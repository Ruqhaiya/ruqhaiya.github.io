<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="In this report, neural networks are employed to identify the bird species of Seattle. We used spectrograms that were derived from Xeno-Canto’s Birdcall competition dataset that consists of 10 high-quality MP3 sound clips for each of the 12 selected bird species[1]. Additionally, three MP3 bird call recordings were available for external testing. The primary goal is to classify bird species based on their distinct vocalizations. We developed two custom neural network models: a binary classification model distinguishing between the American Crow and the Blue Jay, and a multi-class classification model capable of identifying any of the 12 bird species. Predictions on the three external test clips are made to assess the effectiveness of the models. Neural Networks with different architectures and parameters were employed to find the most efficient model.. For hidden layers, various activation functions such as Relu, SoftMax, or Leaky Relu were used. The report concludes with a discussion on alternative modeling approaches and the suitability of neural networks for this specific application.">

<title>Exploring The Sound of Seattle Birds with Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="portfolio-2_files/libs/clipboard/clipboard.min.js"></script>
<script src="portfolio-2_files/libs/quarto-html/quarto.js"></script>
<script src="portfolio-2_files/libs/quarto-html/popper.min.js"></script>
<script src="portfolio-2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="portfolio-2_files/libs/quarto-html/anchor.min.js"></script>
<link href="portfolio-2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="portfolio-2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="portfolio-2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="portfolio-2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="portfolio-2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Exploring The Sound of Seattle Birds with Neural Networks</h1>
</div>

<div>
  <div class="description">
    In this report, neural networks are employed to identify the bird species of Seattle. We used spectrograms that were derived from Xeno-Canto’s Birdcall competition dataset that consists of 10 high-quality MP3 sound clips for each of the 12 selected bird species[1]. Additionally, three MP3 bird call recordings were available for external testing. The primary goal is to classify bird species based on their distinct vocalizations. We developed two custom neural network models: a binary classification model distinguishing between the American Crow and the Blue Jay, and a multi-class classification model capable of identifying any of the 12 bird species. Predictions on the three external test clips are made to assess the effectiveness of the models. Neural Networks with different architectures and parameters were employed to find the most efficient model.. For hidden layers, various activation functions such as Relu, SoftMax, or Leaky Relu were used. The report concludes with a discussion on alternative modeling approaches and the suitability of neural networks for this specific application.
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<ul>
<li><a href="https://github.com/Ruqhaiya/Bird-call-Identification-using-Neural-Networks">Github Code Repository</a></li>
</ul>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>In this report, neural networks are employed to identify the bird species of Seattle. We used spectrograms that were derived from Xeno-Canto’s Birdcall competition dataset that consists of 10 high-quality MP3 sound clips for each of the 12 selected bird species[1]. Additionally, three MP3 bird call recordings were available for external testing. The primary goal is to classify bird species based on their distinct vocalizations. We developed two custom neural network models: a binary classification model distinguishing between the American Crow and the Blue Jay, and a multi-class classification model capable of identifying any of the 12 bird species. Predictions on the three external test clips are made to assess the effectiveness of the models. Neural Networks with different architectures and parameters were employed to find the most efficient model.. For hidden layers, various activation functions such as Relu, SoftMax, or Leaky Relu were used. The report concludes with a discussion on alternative modeling approaches and the suitability of neural networks for this specific application.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this study, a neural network is employed to categorize bird species using pre-processed spectrograms of their sounds. The dataset, originating from Xeno-Canto’s Birdcall competition, comprises original sound clips recorded in the Seattle area. Each clip has been preprocessed into spectrograms, which act as visual representations of the bird sounds and serve as the primary input to the neural networks. These spectrograms are organized in HDF5 format.</p>
<p>The main objective is to develop a robust neural network which can accurately predict species based on their unique sound patterns. We have performed binary as well as multi-class classification tasks on the dataset. We developed various neural network models for the binary classification task to distinguish between the American Crow and the Blue Jay. We used a convolutional neural network for the multi-class classification task that is capable of identifying any of the 12 bird species namely American crow, Barn swallow, Black-capped chickadee, Blue jay, Dark-eyed junco, House-finch, Mallard, Northern flicker, Red-winged blackbird, Stellar’s jay, Western meadowlark, White-crowned sparrow. We have also done a comparative analysis of diverse network structures and hyperparameters to identify the most suitable and efficient model for these kind of classification tasks.</p>
<p>The limitations are discussed in detail for each classification task. Automatic species classification of birds from their sounds has many potential applications in conservation, ecology and archival [2]. The findings of this study can not only help in understanding their unique behavior but also serve as a tool for monitoring bird population and maintaining diversity.</p>
</section>
<section id="theoretical-background" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-background">Theoretical Background</h2>
<p>Neural networks gained popularity in the late 1980s but they didn’t dominate due to their complexity and also the emergence of simpler methods like SVMs. Few years later, with high computational ability and the availability of huge training datasets eventually led to the rise of neural networks under the new name “deep learning.”[3] Neural networks are machine learning algorithms that resemble the structure and functioning of the human brain. They detect complex patterns in data and forecast outcomes based on these patterns. Neural networks as the word suggests are a network of interconnected nodes like the neurons of our brain, they process information in a layered structure and each node passes the information to the next layer of nodes that it is connected to.</p>
<p>Consider the neural network illustrated in Figure 1 as an example. It consists of three layers: an input layer, a hidden layer, and an output layer. The input layer comprises nodes representing four features, namely x1, x2, x3, and x4. There are five activation functions denoted as A1, A2, A3, A4, and A5. These activation functions can be selected from a range of options such as Relu, tanh, sigmoid, softmax, and others. The choice of activation function depends on the specific objectives of the model and the performance characteristics of each function. Careful consideration is necessary to determine the most suitable activation functions for the desired outcomes.</p>
<p>The input layer receives raw data. The output layer is responsible for making the final predictions. The intermediate levels are known as hidden layers, and they perform intermediary calculations. A neural network adjusts the weights between neurons during training to increase its prediction accuracy. The network is fed a set of labeled training data, and an optimization method is used to update the weights such that the network’s predictions are as close to the true labels as possible.</p>
<div data-align="center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/user-attachments/assets/19ed7df1-c4d0-4164-b7a3-6329fd184c85.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>Figure 1 Neural Network with 3 layers- Architecture</p>
</div>
<p>For instance, the Rectified Linear Unit, or ReLU, a popular activation function used in neural networks. It works like this:</p>
<div data-align="center">
<p>f(x) = max(0, x); where x is the input.</p>
</div>
<p>Essentially, ReLU outputs the input value if it’s positive and outputs 0 if it’s negative. This simplicity makes ReLU both computationally efficient, easy to understand and implement. However, it has a potential downside: sometimes, a neuron might always output 0, becoming what we call a ‘dead neuron’.[4] This can happen if the weights are set in such a way that the neuron always receives negative values. To address this, we can use a variation called Leaky ReLU, which allows a small negative slope, keeping the neuron active even with negative inputs.</p>
<p>Neural networks have many applications like image recognition, natural language processing, and speech recognition. They are highly effective at tackling complex problems that traditional algorithms struggle with. Although they can be quite demanding in terms of the computational power and amount of data that is required for training them.</p>
<p>While training a neural network, there are two important things: the optimizer and the loss function. The optimizer decides how weights and biases of the network are adjusted based on the calculated gradients of the loss function. Its goal is to minimize the loss which in turn guides the network to improve its performance. There are various optimizers to choose from, such as Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad, each with unique characteristics and update rules.</p>
<p>On the other hand, the loss function measures the gap between the neural network’s predicted output and the actual output. It measures how well the network is performing on a specific task. Different tasks, like binary classification, multi-class classification, or regression, require different loss functions. For instance, binary cross-entropy works well for binary classification, categorical cross-entropy for multi-class classification, and mean squared error (MSE) for regression tasks.</p>
<p>Convolutional Neural Networks (CNNs) are a specialized type of deep learning model, commonly used for computer vision tasks such as image recognition and classification. CNNs are designed to automatically learn and extract meaningful features from images, making them perfect for handling large volumes of visual data.</p>
<p>At a high level, a CNN is made up of several layers that transform an input image into an output prediction. The first layer usually performs a convolution operation, applying a set of filters to the input image to extract basic features like edges and corners. Subsequent layers might perform pooling operations to downsample the image and reduce its size. This is then followed by additional convolution layers to extract higher-level features. The final output layer then makes a prediction based on the features extracted from the input image.</p>
<p>A pooling layer helps reduce the size of a large image, condensing it into a smaller, summarized version. A common method used is max pooling. It works by summarizing each non-overlapping 2 × 2 block of pixels in an image, using the maximum value within each block.</p>
<p>For example, max pooling looks at each 2 × 2 block and picks the highest value to represent the entire block. This then reduces the size of the image by a factor of two in each direction, significantly lowering the computational load. Additionally, it also provides location invariance: as long as there is one large value within a block, the whole block is represented by that value in the reduced image. The example below explains max pooling in a simpler way.</p>
<div data-align="center">
<p><strong>Input matrix:</strong></p>
<p>[ 1 2 5 3 ]</p>
<p>[ 3 0 4 2 ]</p>
<p>[ 2 1 3 4 ]</p>
<p>[ 1 1 2 0 ]</p>
<p><strong>After max pooling:</strong></p>
<p>[ 3 5 ]</p>
<p>[ 2 4 ]</p>
</div>
<p>CNNs have a great ability to learn features automatically, eliminating the need for manual feature extraction. This makes them highly adaptable to a wide range of tasks, from simple image recognition to more complex tasks like object detection and segmentation. Additionally, CNNs can be trained on large datasets, enabling them to learn highly accurate representations of complex visual patterns.</p>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<p>This research aimed to classify bird data into multiple species using an already preprocessed version of Xeno-Canto’s Birdcall competition dataset.[1] This dataset contained raw audio recordings of bird sounds from 12 distinct species. The recordings were in MP3 format, with varying sampling rates and lengths. The methodology used for pre//processing the sound clips to extract bird calls of a specific species involved several steps. Firstly, the sound clip was subsampled to half its original sample rate, resulting in a new sample rate of 22050 Hz. This step is performed to reduce the computational complexity of subsequent processing steps. Secondly, the “loud” parts of the sound clip are identified, specifically those that are greater than 0.5 seconds in duration. From these identified sections, two second windows of sound are selected where a bird call is detected. These windows are then used to extract the relevant bird call data. To further analyze the bird calls, a spectrogram is produced for each 2-second window. The spectrogram provides a visual representation of the sound in terms of its frequency and intensity over time, resulting in a 343 (time) x 256 (frequency) “image” of the bird call. The spectrograms were labeled with the respective bird species names and saved in an HDF5 file, allowing efficient data access and organization for training and testing purposes.</p>
<p>The dataset was divided into training and test subsets to facilitate model evaluation. Eighty percent of the labeled spectrograms were used for training, while the remaining 20% were allocated to the test set. This ensured that the neural network models could learn from a significant portion of the data while being assessed for their ability to generalize on unseen samples. To allow faster convergence, the standard scaler method was applied to the entire dataset to perform data scaling. The dataset was then trained using various neural networks with multiple hidden layers, and the performance of each model was recorded.</p>
<p>This preprocessing methodology provided well-structured spectrogram data, making it suitable for neural network classification and prediction tasks. For binary classification of American Crow and Blue Jay, we built models with different activation functions for the hidden layers such as ReLu, Leaky ReLu, and Sigmoid. Different activation functions affect the rate at which a network learns. Convolutional Neural Network model was also used with and without dropout regularization and different batch sizes were implemented to train the model. This approach was used to find a suitable and efficient model for this kind of classification task. Similarly, for the multi-class classification, we have implemented a CNN with and without dropout regularization, ReLU(hidden layers) and softmax activation function(output layer).</p>
<p>To improve the model’s performance, several dense layers were then added, each consisting of interconnected neurons. These layers serve as the main processing units of the neural network. The first dense layer contains 32 neurons and uses the Rectified Linear Unit (ReLU) activation function, which helps the model learn complex patterns and relationships in the data. A dropout layer was included after the first dense layer to prevent overfitting, a phenomenon where the model becomes too specialized in the training data and performs poorly on new, unseen data.</p>
<p>To further enhance the model’s performance, additional dense layers with 64 and 128 neurons were added, using ReLU activation and dropout regularization. These layers enable the model to learn more intricate features and capture important characteristics of the input data.</p>
<p>The final layer of the model, called the output layer, is responsible for making predictions. It consists of a number of neurons equal to the number of classes in the dataset. In our case, since we are working with categorical data, the output layer uses the softmax activation function, which calculates the probabilities of each class and assigns the input image to the most probable category.</p>
<p>To train the model, an optimization algorithm called Adam was employed. It adjusts the model’s internal parameters to minimize the error between predicted and actual values. The categorical cross-entropy loss function was chosen to measure the dissimilarity between the predicted class probabilities and the true class labels. The model’s performance was evaluated using the accuracy metric, which indicates how well the model correctly predicts the class of the input images. We have also used early stopping to avoid over learning in the model.</p>
<p>For external testing, we preprocessed three MP3 files using the same methodology that was used for the input data. This allowed for uniformity in input data that the model was trained on. A total of 183 spectrograms were obtained from the test MP3 files. Each file was separately fed to the multi-class CNN classification model to make predictions. The spectrograms were also padded to have the same frequency and time dimensions as the training data.</p>
</section>
<section id="computational-results" class="level2" data-align="center">
<h2 class="anchored" data-anchor-id="computational-results">Computational Results</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 11%">
<col style="width: 22%">
<col style="width: 5%">
<col style="width: 19%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Activation Function (Hidden layers)</th>
<th>Hidden Layers</th>
<th>Dropout Regularization (0.5)</th>
<th>Epochs</th>
<th>Validation Accuracy (%)</th>
<th>CV Error Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sigmoid</td>
<td>1</td>
<td>No</td>
<td>10</td>
<td>80.95</td>
<td>0.40</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>95.24</td>
<td>0.17</td>
</tr>
<tr class="odd">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>90.48</td>
<td>0.32</td>
</tr>
<tr class="even">
<td></td>
<td>1</td>
<td>Yes</td>
<td></td>
<td>95.24</td>
<td>0.19</td>
</tr>
<tr class="odd">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>95.24</td>
<td>0.33</td>
</tr>
<tr class="even">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>47.62</td>
<td>0.67</td>
</tr>
<tr class="odd">
<td>ReLU</td>
<td>1</td>
<td>No</td>
<td></td>
<td>95.24</td>
<td>0.77</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>100</td>
<td>0.06</td>
</tr>
<tr class="odd">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>95.24</td>
<td>0.11</td>
</tr>
<tr class="even">
<td></td>
<td>1</td>
<td>Yes</td>
<td></td>
<td>90.48</td>
<td>0.58</td>
</tr>
<tr class="odd">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>100</td>
<td>0.08</td>
</tr>
<tr class="even">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>76.19</td>
<td>0.34</td>
</tr>
<tr class="odd">
<td>Leaky ReLU</td>
<td>1</td>
<td>No</td>
<td></td>
<td>90.48</td>
<td>0.12</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>95.24</td>
<td>0.09</td>
</tr>
<tr class="odd">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>100</td>
<td>0.07</td>
</tr>
<tr class="even">
<td></td>
<td>1</td>
<td>Yes</td>
<td></td>
<td>95.24</td>
<td>0.11</td>
</tr>
<tr class="odd">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>95.24</td>
<td>0.13</td>
</tr>
<tr class="even">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>100</td>
<td>0.05</td>
</tr>
<tr class="odd">
<td>Tanh</td>
<td>1</td>
<td>No</td>
<td></td>
<td>100</td>
<td>0.21</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>100</td>
<td>0.09</td>
</tr>
<tr class="odd">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>100</td>
<td>0.13</td>
</tr>
<tr class="even">
<td></td>
<td>1</td>
<td>Yes</td>
<td></td>
<td>100</td>
<td>0.08</td>
</tr>
<tr class="odd">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>100</td>
<td>0.21</td>
</tr>
<tr class="even">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>85.71</td>
<td>0.35</td>
</tr>
<tr class="odd">
<td>ReLU - CNN</td>
<td>3</td>
<td>No</td>
<td>10</td>
<td>95.24</td>
<td>0.024</td>
</tr>
<tr class="even">
<td></td>
<td>4</td>
<td>Yes</td>
<td>0.5</td>
<td>20</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td></td>
<td>4</td>
<td></td>
<td>0.3</td>
<td>20</td>
<td>95.24</td>
</tr>
<tr class="even">
<td>CNN - Batch Normalization &amp; Early Stopping</td>
<td>5</td>
<td>Yes</td>
<td>0.3</td>
<td>20</td>
<td>61.90</td>
</tr>
</tbody>
</table>
<p>Figure binary class classification results</p>
</section>
<p><strong>Sigmoid Activation Function</strong>: Models with 1 or 2 hidden layers and no dropout regularization achieved high validation accuracy of upto 94.24% and low CV error rates, suggesting they performed well. Introducing dropout regularization further improved generalization, slightly reducing the CV error rate. However, increasing to 3 hidden layers without dropout led to overfitting, as evidenced by a sharp drop in validation accuracy 47.62% and a high CV error rate.</p>
<p><strong>ReLU Activation Function</strong>: Models with 1 or 2 hidden layers, both with and without dropout, performed excellently with validation accuracies around 95.24% and low CV error rates. The model with 3 hidden layers and no dropout achieved perfect validation accuracy (100%), which is a clear sign of overfitting despite the low CV error rate.</p>
<p><strong>Leaky ReLU Activation Function</strong>: Models with 1 and 2 hidden layers without dropout showed high validation accuracies around 95% and low CV error rates, indicating good performance and generalization. The model with 3 hidden layers also performed exceptionally well, achieving perfect validation accuracy and very low CV error rate, although it looks like it was overfitting.</p>
<p><strong>Tanh Activation Function</strong>: Models with 1 to 3 hidden layers without dropout consistently achieved perfect validation accuracy 100%, suggesting overfitting despite the low CV error rates. Introducing dropout of 0.5 with 2 hidden layers maintained high validation accuracy of 95.24% and a low CV error rate, indicating good performance and generalization.</p>
<p><strong>Convolutional Neural Networks (CNNs)</strong>: The CNN model with ReLU activation, 3 hidden layers, and dropout (0.5) performed well, achieving 95.24% validation accuracy and a very low CV error rate, that shows good generalization of the model classifications. CNN with batch normalization and early stopping showed moderate performance with a validation accuracy of 61.90% and a high CV error rate, indicating possible underfitting.</p>
<p>Below are plots of CNN based binary classification models with various parameters and regularization techniques. The model without dropout regularization tends to overfit, as we can observe the fluctuating validation accuracy and validation loss. Introducing dropout regularization helped the model generalize well as shown by more stable validation accuracy and loss . The dropout rate to 0.3 provides some benefits, but overfitting still occurs. Combining batch normalization and early stopping results in the best performance, with consistent improvements in both training and validation metrics, that shows good generalization of the model.</p>
<div data-align="center">
<p><img src="https://github.com/user-attachments/assets/ef8270a9-2553-471b-ab67-4a71704e2ff0.png" class="img-fluid" alt="image"> <img src="https://github.com/user-attachments/assets/e9d1a3a7-e3a7-4a34-9576-17da0136ad58.png" class="img-fluid" alt="image"></p>
<p><strong>Figure left</strong>: Binary classification CNN model performance without dropout regularization <strong>Right</strong>: CNN model performance with dropout regularization</p>
<p><img src="https://github.com/user-attachments/assets/379e71ad-e853-4c65-ba6d-c6da988e6877.png" class="img-fluid" alt="image"> <img src="https://github.com/user-attachments/assets/0ea9f8b7-79b1-4015-8c9e-62261f1a6ca0.png" class="img-fluid" alt="image"></p>
<p><strong>Figure left</strong>: Binary classification CNN model with 0.3 dropout regularization <strong>Right</strong>: CNN with batch regularization and early stopping</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 11%">
<col style="width: 22%">
<col style="width: 5%">
<col style="width: 19%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Activation Function (Hidden layers)</th>
<th>Hidden Layers</th>
<th>Dropout Regularization (0.5)</th>
<th>Epochs</th>
<th>Validation Accuracy (%)</th>
<th>CV Error Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sigmoid</td>
<td>1</td>
<td>Yes</td>
<td>10</td>
<td>67.24</td>
<td>1.12</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>54.31</td>
<td>1.69</td>
</tr>
<tr class="odd">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>24.14</td>
<td>2.34</td>
</tr>
<tr class="even">
<td>ReLU</td>
<td>1</td>
<td>Yes</td>
<td></td>
<td>62.93</td>
<td>1.30</td>
</tr>
<tr class="odd">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>36.21</td>
<td>1.99</td>
</tr>
<tr class="even">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>12.93</td>
<td>2.38</td>
</tr>
<tr class="odd">
<td>Leaky ReLU</td>
<td>1</td>
<td>Yes</td>
<td></td>
<td>53.45</td>
<td>2.94</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>57.76</td>
<td>1.82</td>
</tr>
<tr class="odd">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>44.83</td>
<td>1.97</td>
</tr>
<tr class="even">
<td>Tanh</td>
<td>1</td>
<td>Yes</td>
<td></td>
<td>63.79</td>
<td>1.12</td>
</tr>
<tr class="odd">
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>55.17</td>
<td>1.42</td>
</tr>
<tr class="even">
<td></td>
<td>3</td>
<td></td>
<td></td>
<td>46.55</td>
<td>1.79</td>
</tr>
<tr class="odd">
<td>ReLU - CNN</td>
<td>3</td>
<td>No</td>
<td></td>
<td>67.24</td>
<td>2.0</td>
</tr>
<tr class="even">
<td></td>
<td>4</td>
<td>Yes</td>
<td></td>
<td>68.97</td>
<td>1.1</td>
</tr>
<tr class="odd">
<td>CNN - Class Weights</td>
<td>4</td>
<td>Yes</td>
<td></td>
<td>69.83</td>
<td>1.01</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>20</td>
<td>72.41</td>
<td>1.06</td>
</tr>
</tbody>
</table>
<p>Figure Multi-class classification models performance metrics</p>
</div>
<p><strong>Sigmoid Activation Function</strong>: With 1 hidden layer and dropout regularization, the model achieved a validation accuracy of 67.24%. Adding more hidden layers (2 and 3) resulted in a decrease in validation accuracy. with increasing CV error rates, indicating overfitting or inadequate learning.</p>
<p><strong>ReLU Activation Function</strong>: The model with 1 hidden layer and dropout regularization achieved a validation accuracy of 62.93%, when the number of hidden layers was increased, there’s a significant drop in performance, with the model’s accuracy falling to ~ 12.93% and high CV error rates, indicating overfitting.</p>
<p><strong>Leaky ReLU Activation Function</strong>: Unlike other models, increasing to 2 and 3 hidden layers showed a slight improvement in validation accuracy to 57.76% and 44.83%, but still had relatively high CV error rates. Tanh Activation Function: The model with 1 hidden layer and dropout regularization performed well with a validation accuracy of 63.79%. However, adding more hidden layers reduced the performance, with validation accuracy dropping to 55.17% and 46.55%, and increased CV error rates.</p>
<p>CNN with 3 hidden layers and no dropout gave a validation accuracy of 67.24% and a CV error rate of 2.0% only. CNN with Class Weights, 4 hidden layers and dropout achieved a validation accuracy of 68.97% and a CV error rate of 1.1%. Another configuration with class weights achieved a validation accuracy of 69.83% and a CV error rate of 1.01%. Extending the training to 20 epochs improved the performance further, with a validation accuracy of 72.41% and a CV error rate of 1.06%.</p>
<div data-align="center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/user-attachments/assets/f37123ff-677c-4119-8778-08487b3f0821.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>Confusion matrix with dropout regularization</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/user-attachments/assets/41001a92-c38e-4514-be71-562dd65cc749.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>Confusion matrix with class weights and 10 epochs</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/user-attachments/assets/7268e4c7-a3fd-4693-8de2-58382df3fff5.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>Confusion matrix with class weights and 20 epochs</p>
</div>
<p>As it is evident, there’s improvement in predictions when we used class weights, this was done because there was class imbalance in the dataset and it led to probabilities of solid 1s and 0s. The models also exhibit overfitting signs due to the imbalance in data. We tried many models and the most efficient were CNNs, although the highest accuracy was 72% for the multi-class classification model.</p>
<div data-align="center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/user-attachments/assets/071a5bd0-26c6-4111-a1cc-330f5be4c2d3.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
</div>
<p>For external testing, when noise reduction was not used, the model struggled to predict the species and we saw 0s and 1s in probabilities. We have used 3 loud segments from the mp3 files and these are called segments. S1,S2,S3 are segments 1,2 and 3. With noise reduction, the model showed great performance. The probabilities can be seen in the table below.</p>
<div data-align="center">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Species Name</th>
<th>Audio File 1 (S1)</th>
<th>Audio File 1 (S2)</th>
<th>Audio File 1 (S3)</th>
<th>Audio File 2 (S1)</th>
<th>Audio File 2 (S2)</th>
<th>Audio File 2 (S3)</th>
<th>Audio File 3 (S1)</th>
<th>Audio File 3 (S2)</th>
<th>Audio File 3 (S3)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>American crow</td>
<td>0.0342</td>
<td>0.0192</td>
<td>0.0640</td>
<td>0.5498</td>
<td>0.0006</td>
<td>0.0208</td>
<td>0.2306</td>
<td>0.0095</td>
<td>0.6532</td>
</tr>
<tr class="even">
<td>Barn swallow</td>
<td>0.8346</td>
<td>0.0127</td>
<td>0.3962</td>
<td>0.0786</td>
<td>0.2246</td>
<td>0.4360</td>
<td>0.1700</td>
<td>0.0202</td>
<td>0.0229</td>
</tr>
<tr class="odd">
<td>Black-capped chickadee</td>
<td>0.0000</td>
<td>0.4753</td>
<td>0.3477</td>
<td>0.1460</td>
<td>0.0047</td>
<td>0.0298</td>
<td>0.0677</td>
<td>0.5501</td>
<td>0.3212</td>
</tr>
<tr class="even">
<td>Blue jay</td>
<td>0.0153</td>
<td>0.0012</td>
<td>0.1311</td>
<td>0.0498</td>
<td>0.6698</td>
<td>0.0334</td>
<td>0.0010</td>
<td>0.0002</td>
<td>0.0002</td>
</tr>
<tr class="odd">
<td>Dark-eyed junco</td>
<td>0.0001</td>
<td>0.3376</td>
<td>0.0023</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0009</td>
<td>0.0050</td>
<td>0.0811</td>
<td>0.0000</td>
</tr>
<tr class="even">
<td>House-finch</td>
<td>0.0000</td>
<td>0.0087</td>
<td>0.0003</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0359</td>
<td>0.0000</td>
</tr>
<tr class="odd">
<td>Mallard</td>
<td>0.0027</td>
<td>0.0032</td>
<td>0.0146</td>
<td>0.0001</td>
<td>0.0002</td>
<td>0.0002</td>
<td>0.0001</td>
<td>0.0007</td>
<td>0.0000</td>
</tr>
<tr class="even">
<td>Northern flicker</td>
<td>0.0000</td>
<td>0.0033</td>
<td>0.0004</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0000</td>
<td>0.0001</td>
<td>0.0000</td>
</tr>
<tr class="odd">
<td>Red-winged blackbird</td>
<td>0.0348</td>
<td>0.0161</td>
<td>0.0126</td>
<td>0.1004</td>
<td>0.0047</td>
<td>0.0510</td>
<td>0.5219</td>
<td>0.3010</td>
<td>0.0003</td>
</tr>
<tr class="even">
<td>Stellar’s jay</td>
<td>0.0657</td>
<td>0.0007</td>
<td>0.0004</td>
<td>0.0017</td>
<td>0.0031</td>
<td>0.0006</td>
<td>0.0026</td>
<td>0.0000</td>
<td>0.0000</td>
</tr>
<tr class="odd">
<td>Western meadowlark</td>
<td>0.0002</td>
<td>0.0729</td>
<td>0.0024</td>
<td>0.0001</td>
<td>0.0000</td>
<td>0.0001</td>
<td>0.0000</td>
<td>0.0001</td>
<td>0.0000</td>
</tr>
<tr class="even">
<td>White-crowned sparrow</td>
<td>0.0126</td>
<td>0.0491</td>
<td>0.0279</td>
<td>0.0734</td>
<td>0.0923</td>
<td>0.4272</td>
<td>0.0011</td>
<td>0.0011</td>
<td>0.0021</td>
</tr>
</tbody>
</table>
<p>Figure results of external testing - probabilities</p>
</div>
<p>The results show that audio files might have more than one species’s vocalization. Which is why we have ranked them based on their descending probabilities below.</p>
<div data-align="center">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Audio File</th>
<th>Dominant Species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Audio File 1</strong></td>
<td>1. Barn swallow</td>
</tr>
<tr class="even">
<td></td>
<td>2. Black-capped chickadee</td>
</tr>
<tr class="odd">
<td></td>
<td>3. Dark-eyed junco</td>
</tr>
<tr class="even">
<td><strong>Audio File 2</strong></td>
<td>1. Blue jay</td>
</tr>
<tr class="odd">
<td></td>
<td>2. American crow</td>
</tr>
<tr class="even">
<td></td>
<td>3. Barn swallow</td>
</tr>
<tr class="odd">
<td><strong>Audio File 3</strong></td>
<td>1. American crow</td>
</tr>
<tr class="even">
<td></td>
<td>2. Black-capped chickadee</td>
</tr>
<tr class="odd">
<td></td>
<td>3. Red-winged blackbird</td>
</tr>
</tbody>
</table>
<p>Figure Dominant species based on aggregated probabilities for each audio file</p>
</div>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>Overall, we explored various neural network architectures with varying functions and parameters. We saw models overfitting and underfitting under different parameters. CNNs have shown a stable performance across both classification problems. We have explored techniques like Batch Normalization and dropout regularization to make the learning process more efficient and reliable. When we notice class imbalance in the dataset, we use class weights to help the model generalize well. This showed significant difference in the model’s performance and that’s evident through the three confusion matrices shown in the computational results. As shown in the results table, especially for binary classification, we can see signs of overfitting where the model wasn’t learning well and achieving high test accuracy even though the training accuracy was low.</p>
<p>And running some of the neural networks with complex architecture took a lot of time. Multi-class classification took almost 8 mins but since the dataset size was not that large, the NNs didn’t take as much time and computational power as they’re expected to. The binary classification models took around 4 mins to train. Early stopping also helped us save time and make the model learn in a more efficient way.</p>
<p>The species that proved most challenging to predict were the House Finch, Mallard, and Northern Flicker, due to consistently low probabilities. Some species, like the Black-capped Chickadee and Blue Jay, or Stellar’s Jay and Red-winged Blackbird, showed potential confusion with one another as we can see in the confusion matrices. The dataset is also not large enough for this kind of bird identification task which requires more training data.</p>
<p>Other models like decision trees, support vector machines, and random forests can be used too. However, a neural network is more suitable for this kind of application because it learns complex data patterns very well compared to traditional models. Neural networks also have the advantage of being able to generalize well to unseen data, making them suitable for classification tasks such as this one.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In conclusion, the study focused on two different classification tasks: Binary Classification, Multi-Class Classification. The main goal was to classify different bird species based on their vocalizations. The study used various deep learning models such as CNN, Sequential CNN with different architectures and parameters along with dropout regularization and batch normalization techniques to combat overfitting. We evaluated their performance based on accuracy and loss metrics. We achieved commendable performance on both binary as well as multi-class classification tasks with such a limited dataset that had a potential class imbalance.</p>
<p>This research contributed to the understanding of classifying bird species based on audio recordings. Despite the challenges faced, the findings provide insights for future enhancements and optimizations in the field of bird species classification using neural networks. With further improvements and expanded datasets, it is possible to achieve more accurate and reliable classification results, thereby contributing to research and conservation efforts.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>[1] Rao, R. (n.d.). Xeno-canto bird recordings extended [Data set]. Kaggle. Retrieved from https://www.kaggle.com/datasets/rohanrao/xeno-canto-bird-recordings-extended-a-m [2] Laiolo, P. (2010). The emerging significance of bioacoustics in animal species conservation. Biological Conservation, 143(7), 1635-1645. https://doi.org/10.1016/j.biocon.2010.03.025. [3] James, G., Witten, D., Hastie, T., Tibshirani, R., &amp; Taylor, J. (2023). An Introduction to Statistical Learning with Applications in Python. (Original work published 2023) https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html [4] Bastian, M. (2019, October 11). Neural Network: The Dead Neuron. Towards Data Science. https://towardsdatascience.com/neural-network-the-dead-neuron-eaa92e575748</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>